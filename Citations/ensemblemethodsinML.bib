
@incollection{goos_ensemble_2000,
	address = {Berlin, Heidelberg},
	title = {Ensemble {Methods} in {Machine} {Learning}},
	volume = {1857},
	isbn = {978-3-540-67704-8 978-3-540-45014-6},
	url = {http://link.springer.com/10.1007/3-540-45014-9_1},
	abstract = {Ensemble methods are learning algorithms that construct a set of classi ers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classi er. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not over t rapidly.},
	language = {en},
	urldate = {2018-12-11},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dietterich, Thomas G.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	year = {2000},
	doi = {10.1007/3-540-45014-9_1},
	pages = {1--15},
	file = {Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:/Users/burakaksar/Zotero/storage/2GRADGHJ/Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:application/pdf}
}